{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"baselines.ipynb","provenance":[],"collapsed_sections":["795pU6wXvmbv","kfZ3Qmp1wY95","MZkS7m49wivU","QmQ36u_AwqRP","UJWug55Mqjv8","6Yn9LRKuqWSR","r91-R9C9qJPj","peNsjkZa_2cF","K1k5X8IDwRE1","ZnGpbfoHl8HS","MoqKSevevdfF","wPvXi5wao10h","9Iqbwyzp1af8","mdsh10r_3Ozk","ZJelBJKq4HmL"],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"795pU6wXvmbv"},"source":["# Installation of required packages"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aEy_J34U9-k2","executionInfo":{"status":"ok","timestamp":1626357300742,"user_tz":-120,"elapsed":242,"user":{"displayName":"Zohaib Akhtar Khan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDxR5SUI_CQuZ4d1iYZsq0ZqAkxd2KNHJ9ahkuVg=s64","userId":"11760862558786327949"}},"outputId":"a94f141c-bc4b-4ac3-b562-4514c50c041c"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t0hB-v3G-ATo","executionInfo":{"status":"ok","timestamp":1626357301100,"user_tz":-120,"elapsed":9,"user":{"displayName":"Zohaib Akhtar Khan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDxR5SUI_CQuZ4d1iYZsq0ZqAkxd2KNHJ9ahkuVg=s64","userId":"11760862558786327949"}},"outputId":"17b5527b-61ea-405f-ed1c-e49ec039f61f"},"source":["cd /content/drive/My Drive/GeSumGenEval"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/.shortcut-targets-by-id/1PcWXs_So5sTaP0wBAR77_ORSfd4aFtHq/GeSumGenEval\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iHSSms0o6zdZ"},"source":["# install all the required packages first after a start of every new collab session\n","!pip install -r requirements.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hQLbRkRYY5z9","executionInfo":{"status":"ok","timestamp":1626357308114,"user_tz":-120,"elapsed":11,"user":{"displayName":"Zohaib Akhtar Khan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDxR5SUI_CQuZ4d1iYZsq0ZqAkxd2KNHJ9ahkuVg=s64","userId":"11760862558786327949"}},"outputId":"13f2b00e-8707-4ece-f684-ffd38484be76"},"source":["import sys\n","import nltk\n","\n","print(sys.executable)\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/bin/python3\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ahxe09ODQElI","executionInfo":{"status":"ok","timestamp":1626357308572,"user_tz":-120,"elapsed":465,"user":{"displayName":"Zohaib Akhtar Khan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDxR5SUI_CQuZ4d1iYZsq0ZqAkxd2KNHJ9ahkuVg=s64","userId":"11760862558786327949"}},"outputId":"e67ade0e-c071-4793-d4bd-14d24febaab6"},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Thu Jul 15 13:55:07 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   41C    P0    34W / 250W |   4631MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kfZ3Qmp1wY95"},"source":["# Data Cleaning"]},{"cell_type":"code","metadata":{"id":"6juln3GJr8YD"},"source":["import re\n","import string\n","#from nltk.corpus import stopwords\n","\n","punctuations = string.punctuation.replace('.', '')\n","#stop_words = stopwords.words(\"german\")\n","def clean_text(x):\n","    # Lowercase the text\n","    x = x.strip().lower()\n","    # Remove stop words\n","    #x = ' '.join([word for word in x.split(' ') if word not in stop_words])\n","    # Remove unicode characters\n","    #x = x.encode('ascii', 'ignore').decode()\n","    # Remove URL\n","    x = re.sub(r'https*\\S+', ' ', x)\n","    # Remove mentions\n","    #x = re.sub(r'@\\S+', ' ', x)\n","    # Remove Hashtags\n","    #x = re.sub(r'#\\S+', ' ', x)\n","    # Remove ticks and the next character\n","    #x = re.sub(r'\\'\\w+', '', x)\n","    # Remove punctuations\n","    x = re.sub('[%s]' % re.escape(punctuations), '', x)\n","    # Remove numbers\n","    #x = re.sub(r'\\w*\\d+\\w*', '', x)\n","    # Replace the over spaces\n","    x = re.sub(r'\\s{2,}', ' ', x)\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MZkS7m49wivU"},"source":["# Summary Generation"]},{"cell_type":"code","metadata":{"id":"oPWiQvhwo7fi"},"source":["import nltk\n","import random\n","\n","def get_random_summary(source: str, num_sent=3, language='german') -> str:\n","    sentences = nltk.sent_tokenize(source, language)\n","\n","    return \"\\n\".join(random.sample(sentences, num_sent))   \n","\n","def get_lead_summary(source: str, num_sent=3, language='german') -> str:\n","    sentences = nltk.sent_tokenize(source, language)\n","\n","    return \"\\n\".join(sentences[:3])\n","\n","from summa.summarizer import summarize\n","\n","def get_textrank_summary(source: str, ratio: float, language='german') -> str:\n","    # By default ratio value is 0.2.\n","    summary = summarize(source, language=language, ratio=ratio)\n","    sentences = nltk.sent_tokenize(summary, language)\n","\n","    return \"\\n\".join(sentences)\n","\n","def get_text_with_breaks(reference: str, language='german') -> str:\n","    sentences = nltk.sent_tokenize(reference, language)\n","\n","    return \"\\n\".join(sentences)\n","\n","def get_word_len(source: str, language='german') -> int:\n","    words = nltk.word_tokenize(source, language)\n","\n","    return len(words)\n","\n","from itertools import combinations\n","def get_oracle_summary(source: str, reference: str, num_sent=3, language='german') -> str:\n","    sentences = nltk.sent_tokenize(source, language)\n","    max_score = 0\n","    oracle_summary = \"\"\n","\n","    candidates = combinations(sentences, num_sent)\n","    for summary in candidates:\n","        summary = \"\\n\".join(summary)\n","        score = get_rouge([summary], [reference], False)[0]['rouge-l']['f']\n","        if score > max_score:\n","            max_score = score\n","            oracle_summary = summary\n","\n","    return oracle_summary"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QmQ36u_AwqRP"},"source":["# Summary Evaluation"]},{"cell_type":"markdown","metadata":{"id":"UJWug55Mqjv8"},"source":["## Evaluation metrics"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WGivYz9zrXCg","executionInfo":{"status":"ok","timestamp":1626357313166,"user_tz":-120,"elapsed":442,"user":{"displayName":"Zohaib Akhtar Khan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDxR5SUI_CQuZ4d1iYZsq0ZqAkxd2KNHJ9ahkuVg=s64","userId":"11760862558786327949"}},"outputId":"7a6690ea-7228-43b6-8688-7527b9e26ea7"},"source":["#! echo $PYTHONPATH\n","%env PYTHONPATH=\"/env/python:/usr/local/lib/python3.7/dist-packages/summ_eval\"\n","#! echo $PYTHONPATH\n","\n","from rouge import Rouge\n","from importlib import reload\n","import src.gerouge as gerouge\n","from summ_eval.bleu_metric import BleuMetric\n","from summ_eval.meteor_metric import MeteorMetric\n","from summ_eval.bert_score_metric import BertScoreMetric\n","from summ_eval.mover_score_metric import MoverScoreMetric\n","import summ_eval.supert_metric as supert_metric\n","from summ_eval.sentence_transformers import SentenceTransformer\n","from blanc import BlancTune\n","from collections import Counter\n","import os\n","\n","spaced_stop_words = \"\"\n","with open('data/smart_stop.txt', 'r', encoding='latin-1') as f:\n","    stop_words = f.read().splitlines()\n","    spaced_stop_words = \" \".join(stop_words)\n","    #spaced_stop_words = spaced_stop_words.decode('latin-1').encode('utf-8')\n","\n","with open('data/spaced_stop_words.txt', 'w', encoding='utf-8') as f:\n","    f.write(spaced_stop_words)\n","\n","def ignore_empty(hyps, refs):\n","    # Filter out hyps of 0 length\n","    hyps_and_refs = zip(hyps, refs)\n","    hyps_and_refs = [_ for _ in hyps_and_refs\n","                        if len(_[0]) > 0\n","                        and len(_[1]) > 0]\n","    \n","    return zip(*hyps_and_refs)\n","\n","def get_rouge(hypothesis, references, avg=True, ignore_empty=True, language='german'):\n","    if language == 'german':\n","        rouge = gerouge.GeRouge(minimal_mode=True)\n","    else:\n","        rouge = Rouge()\n","\n","    rouge_scores = rouge.get_scores(hypothesis, references, avg=avg, ignore_empty=ignore_empty)\n","    if avg:\n","        return {k: v['f'] for k, v in rouge_scores.items()}\n","    else:\n","        return map(list,zip(*[(row['rouge-1']['f'], row['rouge-2']['f'], row['rouge-l']['f']) for row in rouge_scores]))\n","\n","def get_bleu(hypothesis, references, avg=True):\n","    metric = BleuMetric(force=True)\n","    if avg:\n","        hypothesis, references = ignore_empty(hypothesis, references)\n","        bleu_scores = metric.evaluate_batch(hypothesis, references, aggregate=avg)\n","        return bleu_scores['bleu']\n","    else:\n","        bleu_scores = metric.evaluate_batch(hypothesis, references, aggregate=avg)\n","        return [row['bleu'] for row in bleu_scores]\n","    \n","def get_meteor(hypothesis, references, avg=True):\n","    metric = MeteorMetric()\n","    if avg:\n","        hypothesis, references = ignore_empty(hypothesis, references)\n","        meteor_scores = metric.evaluate_batch(hypothesis, references, aggregate=avg)\n","        return meteor_scores['meteor']\n","    else:\n","        meteor_scores = metric.evaluate_batch(hypothesis, references, aggregate=avg)\n","        return [row['meteor'] for row in meteor_scores]\n","\n","def get_bert_score(hypothesis, references, avg=True):\n","    metric = BertScoreMetric(lang='de', model_type='dbmdz/bert-base-german-cased', num_layers=9, verbose=False, idf=True, rescale_with_baseline=False)\n","    if avg:\n","        hypothesis, references = ignore_empty(hypothesis, references)\n","        bert_scores = metric.evaluate_batch(hypothesis, references, aggregate=avg)\n","        return bert_scores['bert_score_f1']\n","    else:\n","        bert_scores = metric.evaluate_batch(hypothesis, references, aggregate=avg)\n","        return [row['bert_score_f1'] for row in bert_scores]\n","\n","def get_mover_score(hypothesis, references, avg=True):\n","    os.environ['MOVERSCORE_MODEL'] = \"dbmdz/bert-base-german-cased\"\n","    metric = MoverScoreMetric(version=2, stop_wordsf='data/spaced_stop_words.txt')\n","    if avg:\n","        hypothesis, references = ignore_empty(hypothesis, references)\n","        mover_scores = metric.evaluate_batch(hypothesis, references, aggregate=avg)\n","        return mover_scores['mover_score']\n","    else:\n","        mover_scores = metric.evaluate_batch(hypothesis, references, aggregate=avg)\n","        return [row['mover_score'] for row in mover_scores]\n","\n","def get_blanc(hypothesis, sources, avg=True):\n","    corpus_score_dict = Counter()\n","    # Best configuration parameter values for german language taken from https://arxiv.org/abs/2105.06027\n","    blanc_mod = BlancTune(device='cuda', inference_batch_size=128, finetune_batch_size=24, model_name='dbmdz/bert-base-german-cased', gap=2, \n","                          min_token_length_normal=4, min_token_length_lead=2, min_token_length_followup=1)\n","        \n","    \n","    if avg:\n","        hypothesis, sources = ignore_empty(hypothesis, sources)\n","        results = blanc_mod.eval_pairs(sources, hypothesis)\n","        results = [{\"blanc\": score} for score in results]\n","        [corpus_score_dict.update(x) for x in results]\n","        for key in corpus_score_dict.keys():\n","            corpus_score_dict[key] /= float(len(sources))\n","        return corpus_score_dict['blanc']\n","    else:\n","        results = blanc_mod.eval_pairs(sources, hypothesis)\n","        results = [{\"blanc\": score} for score in results]\n","        return [row['blanc'] for row in results]\n","\n","def get_supert(hypothesis, sources, avg=True):\n","    metric = supert_metric.SupertMetric()\n","    #metric.bert_model = SentenceTransformer('xlm-r-100langs-bert-base-nli-stsb-mean-tokens')\n","    if avg:\n","        hypothesis, sources = ignore_empty(hypothesis, sources)\n","        supert_scores = metric.evaluate_batch(hypothesis, sources, aggregate=avg)\n","        return supert_scores['supert']\n","    else:\n","        supert_scores = metric.evaluate_batch(hypothesis, sources, aggregate=avg)\n","        return [row['supert'] for row in supert_scores]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["env: PYTHONPATH=\"/env/python:/usr/local/lib/python3.7/dist-packages/summ_eval\"\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6Yn9LRKuqWSR"},"source":["## Quality Estimation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7T2j6gpvqZZW","executionInfo":{"status":"ok","timestamp":1626359926652,"user_tz":-120,"elapsed":1718,"user":{"displayName":"Zohaib Akhtar Khan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDxR5SUI_CQuZ4d1iYZsq0ZqAkxd2KNHJ9ahkuVg=s64","userId":"11760862558786327949"}},"outputId":"8029d748-2b1c-4044-8220-6d7e833abd59"},"source":["import pandas as pd\n","from transformers import BertTokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","import torch\n","import numpy as np\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")\n","\n","# Load the BERT tokenizer.\n","print('Loading BERT tokenizer...')\n","tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-german-cased')\n","MAX_LEN = 200\n","\n","def get_qe(hypothesis, avg=True):\n","    output = {\n","        'expert_coherence': [],\n","        'expert_consistency': [],\n","        'expert_fluency': [],\n","        'expert_relevance': [],\n","        'crowd_coherence': [],\n","        'crowd_consistency': [],\n","        'crowd_fluency': [],\n","        'crowd_relevance': [],\n","        }\n","\n","    for key, value in output.items():\n","        # Model class must be defined somewhere\n","        model = torch.load(f'models/cnndm/{key}.pt')\n","        model.eval()\n","\n","        # Tokenize all of the sentences and map the tokens to thier word IDs.\n","        input_ids = []\n","\n","        # For every sentence...\n","        for sent in hypothesis:\n","            # `encode` will:\n","            #   (1) Tokenize the sentence.\n","            #   (2) Prepend the `[CLS]` token to the start.\n","            #   (3) Append the `[SEP]` token to the end.\n","            #   (4) Map tokens to their IDs.\n","            encoded_sent = tokenizer.encode(\n","                                sent,                      # Sentence to encode.\n","                                add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        )\n","            \n","            input_ids.append(encoded_sent)\n","\n","        # Pad our input tokens\n","        input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n","                                dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","        # Create attention masks\n","        attention_masks = []\n","\n","        # Create a mask of 1s for each token followed by 0s for padding\n","        for seq in input_ids:\n","            seq_mask = [float(i>0) for i in seq]\n","            attention_masks.append(seq_mask) \n","\n","        # Convert to tensors.\n","        prediction_inputs = torch.tensor(input_ids)\n","        prediction_masks = torch.tensor(attention_masks)\n","\n","        # Set the batch size.  \n","        batch_size = 16  \n","\n","        # Create the DataLoader.\n","        prediction_data = TensorDataset(prediction_inputs, prediction_masks)\n","        prediction_sampler = SequentialSampler(prediction_data)\n","        prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n","\n","        # Prediction on test set\n","\n","        print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n","\n","        # Predict \n","        for batch in prediction_dataloader:\n","            # Add batch to GPU\n","            batch = tuple(t.to(device) for t in batch)\n","            \n","            # Unpack the inputs from our dataloader\n","            b_input_ids, b_input_mask = batch\n","            \n","            # Telling the model not to compute or store gradients, saving memory and \n","            # speeding up prediction\n","            with torch.no_grad():\n","                # Forward pass, calculate logit predictions\n","                outputs = model(b_input_ids, token_type_ids=None, \n","                                attention_mask=b_input_mask)\n","\n","            logits = outputs[0]\n","\n","            # Move logits to CPU\n","            logits = logits.detach().cpu().numpy()\n","            # Store predictions\n","            pred_labels_i = np.argmax(logits, axis=1).flatten()\n","            value.extend(pred_labels_i + 1)\n","\n","    return output"],"execution_count":null,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla P100-PCIE-16GB\n","Loading BERT tokenizer...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"r91-R9C9qJPj"},"source":["## Jensen Shannon"]},{"cell_type":"code","metadata":{"id":"m1yZkP4JmFI3"},"source":["from nltk.stem.snowball import GermanStemmer\n","import re\n","import copy\n","import scipy\n","from statistics import mean\n","\n","with open('data/spaced_stop_words.txt', 'r', encoding='utf-8') as f:\n","    STOP_WORDS = set(f.read().strip().split(' '))\n","    \n","porter = GermanStemmer()\n","\n","def text_to_distribution(txt, do_stem_porter=False, remove_stopwords=False):\n","    words = re.findall(r\"[\\w']+|[.,!?;]\", txt)\n","    if do_stem_porter:\n","        words = [porter.stem(w) for w in words]\n","    if remove_stopwords:\n","        words = [w for w in words if w not in STOP_WORDS]\n","    map_word_count = {}\n","    for w in words:\n","        if w in map_word_count:\n","            map_word_count[w] += 1\n","        else:\n","            map_word_count[w] = 1\n","    return map_word_count\n","\n","\n","def combine_distributions(distr1, distr2):\n","    d1 = copy.deepcopy(distr1)\n","    for k in distr2.keys():\n","        if k not in d1:\n","            d1[k] = 0\n","    d2 = copy.deepcopy(distr2)\n","    for k in distr1.keys():\n","        if k not in d2:\n","            d2[k] = 0\n","    keys = d1.keys()\n","    v1 = [d1[k] for k in keys]\n","    v2 = [d2[k] for k in keys]\n","    return v1, v2\n","\n","\n","def good_len_summ(summary, low=-1, high=1000000):\n","    summ = ' '.join(summary.strip().split())\n","    len_summ = len(summ)\n","    if len(summ) >= low and len(summ)<=high:\n","        return True\n","    return False\n","\n","def js_divergence(hypothesis, sources, low=-1, high=1000000, do_stem_porter=True, remove_stopwords=True):\n","    divergences = []\n","    for i in range(len(hypothesis)):\n","        summ = hypothesis[i]\n","        if not good_len_summ(summ, low=low, high=high):\n","            continue\n","        text = sources[i]\n","        distr_summ = text_to_distribution(summ, do_stem_porter=do_stem_porter, remove_stopwords=remove_stopwords)\n","        distr_text = text_to_distribution(text, do_stem_porter=do_stem_porter, remove_stopwords=remove_stopwords)\n","        v1, v2 = combine_distributions(distr_summ, distr_text)\n","        divergence = scipy.spatial.distance.jensenshannon(v1, v2)**2\n","        divergences.append(divergence)\n","\n","    return divergences\n","\n","def get_jensenshannon(hypothesis, sources, avg=True):\n","    if avg:\n","        hypothesis, sources = ignore_empty(hypothesis, sources)\n","        js_scores = js_divergence(hypothesis, sources)\n","        return mean(js_scores)\n","    else:\n","        js_scores = js_divergence(hypothesis, sources)\n","        return js_scores"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"peNsjkZa_2cF"},"source":["# Data Preprocessing for BertSum"]},{"cell_type":"code","metadata":{"id":"f515bTAxAD8q"},"source":["import nltk\n","import json\n","import pandas as pd\n","\n","def data_prep_for_BertSum(dataset: pd.DataFrame, language: str, save_path: str, corpus_type: str) -> None:\n","    print(f\"Sentence splitting, tokenizing and converting '{corpus_type}' split to json...\")\n","    dataset_json = []\n","    p_ct = 0\n","    shard_size = 2000\n","    for index, row in dataset.iterrows():\n","        src_tokens = []\n","        tgt_tokens = []\n","\n","        src_sentences = nltk.sent_tokenize(row['text'], language)\n","        for sent in src_sentences:\n","            src_tokens.append(nltk.word_tokenize(sent, language))\n","\n","        tgt_sentences = nltk.sent_tokenize(row['summary'], language)\n","        for sent in tgt_sentences:\n","            tgt_tokens.append(nltk.word_tokenize(sent, language))\n","\n","        dataset_json.append({'src': src_tokens, 'tgt': tgt_tokens})\n","        if (len(dataset_json) >= shard_size):\n","                pt_file = \"{:s}/{:s}.{:d}.json\".format(save_path, corpus_type, p_ct)\n","                with open(pt_file, 'w') as save:\n","                    # save.write('\\n'.join(dataset_json))\n","                    save.write(json.dumps(dataset_json))\n","                    p_ct += 1\n","                    dataset_json = []\n","\n","    if (len(dataset_json) > 0):\n","        pt_file = \"{:s}/{:s}.{:d}.json\".format(save_path, corpus_type, p_ct)\n","        with open(pt_file, 'w') as save:\n","            # save.write('\\n'.join(dataset_json))\n","            save.write(json.dumps(dataset_json))\n","            p_ct += 1\n","            dataset_json = []"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K1k5X8IDwRE1"},"source":["# Data Preprocessing for MatchSum"]},{"cell_type":"code","metadata":{"id":"bQ8Gc1t0wapU"},"source":["import nltk\n","import json\n","import pandas as pd\n","\n","def data_prep_for_MatchSum(dataset: pd.DataFrame, language: str, save_path: str, corpus_type: str) -> None:\n","    print(f\"Sentence splitting and converting '{corpus_type}' split to json...\")\n","\n","    pt_file = \"{:s}/{:s}.jsonl\".format(save_path, corpus_type)\n","    with open(pt_file, 'w') as save:\n","        for index, row in dataset.iterrows():\n","            src_sentences = nltk.sent_tokenize(row['text'], language)\n","            tgt_sentences = nltk.sent_tokenize(row['summary'], language)\n","            save.write(json.dumps({'text': src_sentences, 'summary': tgt_sentences}))\n","            save.write(\"\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZnGpbfoHl8HS"},"source":["# MLSUM"]},{"cell_type":"markdown","metadata":{"id":"MoqKSevevdfF"},"source":["## Data Loading"]},{"cell_type":"code","metadata":{"id":"R54nPMUeQhDE"},"source":["# Let's import the library. We typically only need at most four methods:\n","from datasets import list_datasets, list_metrics, load_dataset, load_metric\n","from pprint import pprint\n","import pandas as pd\n","\n","def load_mlsum_to_csv(corpus_type: str) -> pd.DataFrame:\n","    # Downloading and loading a dataset\n","    hf_split: str = corpus_type\n","    if hf_split == \"valid\":\n","        hf_split = \"validation\"\n","    mlsum_dataset = load_dataset('mlsum', 'de', split=hf_split)\n","\n","    # Saving dataframe in the form of csv\n","    df = pd.DataFrame(mlsum_dataset, columns=[\"text\",\"summary\"])\n","    #df.to_csv(f\"data/mlsum/{corpus_type}.csv\", index=False)\n","\n","    return df\n","\n","#for corpus_type in ['train', 'valid', 'test']:\n","#    mlsum_dataset = load_mlsum_to_csv(corpus_type)\n","#    data_prep_for_BertSum(mlsum_dataset, 'german', \"json_data/mlsum\", corpus_type)\n","#    data_prep_for_MatchSum(mlsum_dataset, 'german', \"json_data/mlsum\", corpus_type)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wVCgvT8d7l8M","executionInfo":{"status":"ok","timestamp":1626357561872,"user_tz":-120,"elapsed":238740,"user":{"displayName":"Zohaib Akhtar Khan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDxR5SUI_CQuZ4d1iYZsq0ZqAkxd2KNHJ9ahkuVg=s64","userId":"11760862558786327949"}},"outputId":"4e4d6101-5bac-4b5f-8851-9e90d3f85723"},"source":["import itertools\n","import pandas as pd\n","\n","#merged_mlsum_dataset = itertools.chain(mlsum_dataset['train'], mlsum_dataset['test'], mlsum_dataset['validation'])\n","mlsum_dataset = pd.read_csv(\"data/mlsum/test.csv\")\n","mlsum_src, mlsum_rnd_sum, mlsum_lead_sum, mlsum_textrank_sum, mlsum_tgt, mlsum_dataset[\"text_word_len\"], mlsum_dataset[\"sum_word_len\"] = map(list,zip(*[(\n","                                                      get_text_with_breaks(row['text']),\n","                                                      get_random_summary(row['text']),\n","                                                      get_lead_summary(row['text']),\n","                                                      get_textrank_summary(get_text_with_breaks(row['text']), 0.06),\n","                                                      get_text_with_breaks(row['summary']),\n","                                                      get_word_len(row['text']), \n","                                                      get_word_len(row['summary']) \n","                                                    ) for index, row in mlsum_dataset.iterrows()]))\n","print(len(mlsum_src))\n","print(len(mlsum_rnd_sum))\n","print(len(mlsum_lead_sum))\n","print(len(mlsum_textrank_sum))\n","print(len(mlsum_tgt))\n","\n","#pd.DataFrame(mlsum_src).to_csv(\"results/mlsum/source.csv\", index=False)\n","#pd.DataFrame(mlsum_rnd_sum).to_csv(\"results/mlsum/random_hypo.csv\", index=False)\n","#pd.DataFrame(mlsum_lead_sum).to_csv(\"results/mlsum/lead_hypo.csv\", index=False)\n","#pd.DataFrame(mlsum_textrank_sum).to_csv(\"results/mlsum/textrank_hypo.csv\", index=False)\n","#pd.DataFrame(mlsum_tgt).to_csv(\"results/mlsum/reference.csv\", index=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["10701\n","10701\n","10701\n","10701\n","10701\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4awNdHuqsgr7","executionInfo":{"status":"ok","timestamp":1626357561873,"user_tz":-120,"elapsed":10,"user":{"displayName":"Zohaib Akhtar Khan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDxR5SUI_CQuZ4d1iYZsq0ZqAkxd2KNHJ9ahkuVg=s64","userId":"11760862558786327949"}},"outputId":"4ae87b12-f157-4b5a-987e-efa900b3a5ce"},"source":["import pandas as pd\n","\n","mlsum_oracle_sum = pd.read_csv(\"results/mlsum/oracle_hypo.csv\")['hypothesis'].fillna('').tolist()\n","mlsum_oracle_tgt = pd.read_csv(\"results/mlsum/oracle_ref.csv\")['references'].fillna('').tolist()\n","mlsum_bertsum_sum = pd.read_csv(\"results/mlsum/bertsum_hypo.csv\")['hypothesis'].fillna('').tolist()\n","mlsum_bertsum_tgt = pd.read_csv(\"results/mlsum/bertsum_ref.csv\")['references'].fillna('').tolist()\n","mlsum_matchsum_sum = pd.read_csv(\"results/mlsum/matchsum_hypo.csv\")['hypothesis'].fillna('').tolist()\n","mlsum_matchsum_tgt = pd.read_csv(\"results/mlsum/matchsum_ref.csv\")['references'].fillna('').tolist()\n","\n","print(len(mlsum_oracle_sum))\n","print(len(mlsum_oracle_tgt))\n","print(len(mlsum_bertsum_sum))\n","print(len(mlsum_bertsum_tgt))\n","print(len(mlsum_matchsum_sum))\n","print(len(mlsum_matchsum_tgt))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["10701\n","10701\n","10701\n","10701\n","10701\n","10701\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"geiA5rYgpDBm"},"source":["## Individual evaluation results per evaluation method"]},{"cell_type":"code","metadata":{"id":"rAg6OX5fpdTx"},"source":["import pandas as pd\n","\n","r1_rnd, r2_rnd, rl_rnd = get_rouge(mlsum_rnd_sum, mlsum_tgt, avg=False, ignore_empty=False)\n","r1_lead, r2_lead, rl_lead = get_rouge(mlsum_lead_sum, mlsum_tgt, avg=False, ignore_empty=False)\n","r1_tr, r2_tr, rl_tr = get_rouge(mlsum_textrank_sum, mlsum_tgt, avg=False, ignore_empty=False)\n","r1_bs, r2_bs, rl_bs = get_rouge(mlsum_bertsum_sum, mlsum_bertsum_tgt, avg=False, ignore_empty=False)\n","r1_ms, r2_ms, rl_ms = get_rouge(mlsum_matchsum_sum, mlsum_matchsum_tgt, avg=False, ignore_empty=False)\n","r1_oracle, r2_oracle, rl_oracle = get_rouge(mlsum_oracle_sum, mlsum_oracle_tgt, avg=False, ignore_empty=False)\n","\n","rouge_eval = pd.DataFrame({\n","    'r1-Random-3': r1_rnd, 'r2-Random-3': r2_rnd, 'rl-Random-3': rl_rnd,\n","    'r1-Lead-3': r1_lead, 'r2-Lead-3': r2_lead, 'rl-Lead-3': rl_lead,\n","    'r1-TextRank': r1_tr, 'r2-TextRank': r2_tr, 'rl-TextRank': rl_tr,\n","    'r1-BertSum': r1_bs, 'r2-BertSum': r2_bs, 'rl-BertSum': rl_bs,\n","    'r1-MatchSum': r1_ms, 'r2-Matchsum': r2_ms, 'rl-MatchSum': rl_ms,\n","    'r1-Oracle': r1_oracle, 'r2-Oracle': r2_oracle, 'rl-Oracle': rl_oracle\n","})\n","rouge_eval.to_csv(\"results/mlsum/rouge_eval.csv\", index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vkhnqHPwsLze"},"source":["bleu_eval = pd.DataFrame({\n","    'Random-3': get_bleu(mlsum_rnd_sum, mlsum_tgt, avg=False),\n","    'Lead-3': get_bleu(mlsum_lead_sum, mlsum_tgt, avg=False),\n","    'TextRank': get_bleu(mlsum_textrank_sum, mlsum_tgt, avg=False),\n","    'BertSum': get_bleu(mlsum_bertsum_sum, mlsum_bertsum_tgt, avg=False),\n","    'MatchSum': get_bleu(mlsum_matchsum_sum, mlsum_matchsum_tgt, avg=False),\n","    'Oracle': get_bleu(mlsum_oracle_sum, mlsum_oracle_tgt, avg=False)\n","})\n","bleu_eval.to_csv(\"results/mlsum/bleu_eval.csv\", index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T1AIywHcuTe2"},"source":["meteor_eval = pd.DataFrame({\n","    'Random-3': get_meteor(mlsum_rnd_sum, mlsum_tgt, avg=False),\n","    'Lead-3': get_meteor(mlsum_lead_sum, mlsum_tgt, avg=False),\n","    'TextRank': get_meteor(mlsum_textrank_sum, mlsum_tgt, avg=False),\n","    'BertSum': get_meteor(mlsum_bertsum_sum, mlsum_bertsum_tgt, avg=False),\n","    'MatchSum': get_meteor(mlsum_matchsum_sum, mlsum_matchsum_tgt, avg=False),\n","    'Oracle': get_meteor(mlsum_oracle_sum, mlsum_oracle_tgt, avg=False)\n","})\n","meteor_eval.to_csv(\"results/mlsum/meteor_eval.csv\", index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aydPsjANuqbE"},"source":["bert_score_eval = pd.DataFrame({\n","    'Random-3': get_bert_score(mlsum_rnd_sum, mlsum_tgt, avg=False),\n","    'Lead-3': get_bert_score(mlsum_lead_sum, mlsum_tgt, avg=False),\n","    'TextRank': get_bert_score(mlsum_textrank_sum, mlsum_tgt, avg=False),\n","    'BertSum': get_bert_score(mlsum_bertsum_sum, mlsum_bertsum_tgt, avg=False),\n","    'MatchSum': get_bert_score(mlsum_matchsum_sum, mlsum_matchsum_tgt, avg=False),\n","    'Oracle': get_bert_score(mlsum_oracle_sum, mlsum_oracle_tgt, avg=False)\n","})\n","bert_score_eval.to_csv(\"results/mlsum/bert_score_eval.csv\", index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RoWXgM242w0E"},"source":["mover_score_eval = pd.DataFrame({\n","    'Random-3': get_mover_score(mlsum_rnd_sum, mlsum_tgt, avg=False),\n","    'Lead-3': get_mover_score(mlsum_lead_sum, mlsum_tgt, avg=False),\n","    'TextRank': get_mover_score(mlsum_textrank_sum, mlsum_tgt, avg=False),\n","    'BertSum': get_mover_score(mlsum_bertsum_sum, mlsum_bertsum_tgt, avg=False),\n","    'MatchSum': get_mover_score(mlsum_matchsum_sum, mlsum_matchsum_tgt, avg=False),\n","    'Oracle': get_mover_score(mlsum_oracle_sum, mlsum_oracle_tgt, avg=False)\n","})\n","mover_score_eval.to_csv(\"results/mlsum/mover_score_eval.csv\", index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HXm2X4pwqLkp"},"source":["blanc_eval = pd.DataFrame({\n","    'Random-3': get_blanc(mlsum_rnd_sum, mlsum_src, avg=False),\n","    'Lead-3': get_blanc(mlsum_lead_sum, mlsum_src, avg=False),\n","    'TextRank': get_blanc(mlsum_textrank_sum, mlsum_src, avg=False),\n","    'BertSum': get_blanc(mlsum_bertsum_sum, mlsum_src, avg=False),\n","    'MatchSum': get_blanc(mlsum_matchsum_sum, mlsum_src, avg=False),\n","    'Oracle': get_blanc(mlsum_oracle_sum, mlsum_src, avg=False)\n","})\n","blanc_eval.to_csv(\"results/mlsum/blanc_eval.csv\", index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"08i2nV_j3cVg"},"source":["js_eval = pd.DataFrame({\n","    'Random-3': get_jensenshannon(mlsum_rnd_sum, mlsum_src, avg=False),\n","    'Lead-3': get_jensenshannon(mlsum_lead_sum, mlsum_src, avg=False),\n","    'TextRank': get_jensenshannon(mlsum_textrank_sum, mlsum_src, avg=False),\n","    'BertSum': get_jensenshannon(mlsum_bertsum_sum, mlsum_src, avg=False),\n","    'MatchSum': get_jensenshannon(mlsum_matchsum_sum, mlsum_src, avg=False),\n","    'Oracle': get_jensenshannon(mlsum_oracle_sum, mlsum_src, avg=False)\n","})\n","js_eval.to_csv(\"results/mlsum/js_eval.csv\", index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GpU1w7NJAe2H"},"source":["% cd /usr/local/lib/python3.7/dist-packages/summ_eval/\n","supert_eval = pd.DataFrame({\n","    'Random-3': get_supert(mlsum_rnd_sum, mlsum_src, avg=False),\n","    'Lead-3': get_supert(mlsum_lead_sum, mlsum_src, avg=False),\n","    'TextRank': get_supert(mlsum_textrank_sum, mlsum_src, avg=False),\n","    'BertSum': get_supert(mlsum_bertsum_sum, mlsum_src, avg=False),\n","    'MatchSum': get_supert(mlsum_matchsum_sum, mlsum_src, avg=False),\n","    'Oracle': get_supert(mlsum_oracle_sum, mlsum_src, avg=False)\n","})\n","% cd /content/drive/My Drive/GeSumGenEval\n","supert_eval.to_csv(\"results/mlsum/supert_eval.csv\", index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yu4NGuvezdSa"},"source":["#qe_rnd = get_qe(mlsum_rnd_sum, avg=False)\n","qe_lead = get_qe(mlsum_lead_sum, avg=False)\n","qe_tr = get_qe(mlsum_textrank_sum, avg=False)\n","qe_bs = get_qe(mlsum_bertsum_sum, avg=False)\n","qe_ms = get_qe(mlsum_matchsum_sum, avg=False)\n","qe_oracle = get_qe(mlsum_oracle_sum, avg=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jr5_t4XB0r4Y"},"source":["quality_dim = [\n","        'expert_coherence',\n","        'expert_consistency',\n","        'expert_fluency',\n","        'expert_relevance',\n","        'crowd_coherence',\n","        'crowd_consistency',\n","        'crowd_fluency',\n","        'crowd_relevance',\n","]\n","qe_eval = pd.DataFrame()\n","for dim in quality_dim:\n","    qe_eval['Random-3 (' + dim + ')'] = qe_rnd[dim]\n","    qe_eval['Lead-3 (' + dim + ')'] = qe_lead[dim]\n","    qe_eval['TextRank (' + dim + ')'] = qe_tr[dim]\n","    qe_eval['BertSum (' + dim + ')'] = qe_bs[dim]\n","    qe_eval['MatchSum (' + dim + ')'] = qe_ms[dim]\n","    qe_eval['Oracle (' + dim + ')'] = qe_oracle[dim]\n","qe_eval.to_csv(\"results/mlsum/qe_eval.csv\", index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wPvXi5wao10h"},"source":["## Aggregate evaluation results per generation method"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gwdr4phj5hya","executionInfo":{"elapsed":9,"status":"ok","timestamp":1625074004023,"user":{"displayName":"Zohaib Akhtar Khan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDxR5SUI_CQuZ4d1iYZsq0ZqAkxd2KNHJ9ahkuVg=s64","userId":"11760862558786327949"},"user_tz":-120},"outputId":"4ec824d8-d337-4e75-baff-d5f0a944d520"},"source":["% cd /usr/local/lib/python3.7/dist-packages/summ_eval/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/summ_eval\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mPYaAUXc7qCE"},"source":["oracle_rouge = get_rouge(mlsum_oracle_sum, mlsum_oracle_tgt)\n","oracle_bleu = get_bleu(mlsum_oracle_sum, mlsum_oracle_tgt)\n","oracle_meteor = get_meteor(mlsum_oracle_sum, mlsum_oracle_tgt)\n","oracle_bert_score = get_bert_score(mlsum_oracle_sum, mlsum_oracle_tgt)\n","oracle_mover_score = get_mover_score(mlsum_oracle_sum, mlsum_oracle_tgt)\n","oracle_blanc = get_blanc(mlsum_oracle_sum, mlsum_src)\n","oracle_js = get_jensenshannon(mlsum_oracle_sum, mlsum_src)\n","oracle_supert = get_supert(mlsum_oracle_sum, mlsum_src)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_YInfLYb7rNZ"},"source":["bertsum_rouge = get_rouge(mlsum_bertsum_sum, mlsum_bertsum_tgt)\n","bertsum_bleu = get_bleu(mlsum_bertsum_sum, mlsum_bertsum_tgt)\n","bertsum_meteor = get_meteor(mlsum_bertsum_sum, mlsum_bertsum_tgt)\n","bertsum_bert_score = get_bert_score(mlsum_bertsum_sum, mlsum_bertsum_tgt)\n","bertsum_mover_score = get_mover_score(mlsum_bertsum_sum, mlsum_bertsum_tgt)\n","bertsum_blanc = get_blanc(mlsum_bertsum_sum, mlsum_src)\n","bertsum_js = get_jensenshannon(mlsum_bertsum_sum, mlsum_src)\n","bertsum_supert = get_supert(mlsum_bertsum_sum, mlsum_src)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zKBhYl3Oq57f"},"source":["matchsum_rouge = get_rouge(mlsum_matchsum_sum, mlsum_matchsum_tgt)\n","matchsum_bleu = get_bleu(mlsum_matchsum_sum, mlsum_matchsum_tgt)\n","matchsum_meteor = get_meteor(mlsum_matchsum_sum, mlsum_matchsum_tgt)\n","matchsum_bert_score = get_bert_score(mlsum_matchsum_sum, mlsum_matchsum_tgt)\n","matchsum_mover_score = get_mover_score(mlsum_matchsum_sum, mlsum_matchsum_tgt)\n","matchsum_blanc = get_blanc(mlsum_matchsum_sum, mlsum_src)\n","matchsum_js = get_jensenshannon(mlsum_matchsum_sum, mlsum_src)\n","matchsum_supert = get_supert(mlsum_matchsum_sum, mlsum_src)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5PBz9lKV-Aus"},"source":["rnd_rouge = get_rouge(mlsum_rnd_sum, mlsum_tgt)\n","rnd_bleu = get_bleu(mlsum_rnd_sum, mlsum_tgt)\n","rnd_meteor = get_meteor(mlsum_rnd_sum, mlsum_tgt)\n","rnd_bert_score = get_bert_score(mlsum_rnd_sum, mlsum_tgt)\n","rnd_mover_score = get_mover_score(mlsum_rnd_sum, mlsum_tgt)\n","rnd_blanc = get_blanc(mlsum_rnd_sum, mlsum_src)\n","rnd_js = get_jensenshannon(mlsum_rnd_sum, mlsum_src)\n","rnd_supert = get_supert(mlsum_rnd_sum, mlsum_src)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V_hV5A1Dw-BH"},"source":["lead_rouge =  get_rouge(mlsum_lead_sum, mlsum_tgt)\n","lead_bleu = get_bleu(mlsum_lead_sum, mlsum_tgt)\n","lead_meteor = get_meteor(mlsum_lead_sum, mlsum_tgt)\n","lead_bert_score = get_bert_score(mlsum_lead_sum, mlsum_tgt)\n","lead_mover_score = get_mover_score(mlsum_lead_sum, mlsum_tgt)\n","lead_blanc = get_blanc(mlsum_lead_sum, mlsum_src)\n","lead_js = get_jensenshannon(mlsum_lead_sum, mlsum_src)\n","lead_supert = get_supert(mlsum_lead_sum, mlsum_src)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"95CnXePcxx7I"},"source":["tr_rouge = get_rouge(mlsum_textrank_sum, mlsum_tgt)\n","tr_bleu = get_bleu(mlsum_textrank_sum, mlsum_tgt)\n","tr_meteor = get_meteor(mlsum_textrank_sum, mlsum_tgt)\n","tr_bert_score = get_bert_score(mlsum_textrank_sum, mlsum_tgt)\n","tr_mover_score = get_mover_score(mlsum_textrank_sum, mlsum_tgt)\n","tr_blanc = get_blanc(mlsum_textrank_sum, mlsum_src)\n","tr_js = get_jensenshannon(mlsum_textrank_sum, mlsum_src)\n","tr_supert = get_supert(mlsum_textrank_sum, mlsum_src)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PRGad9OqjBey"},"source":["import pandas as pd\n","\n","mlsum_eval_df = pd.DataFrame([\n","    [\"Random-3\",rnd_rouge['rouge-1'],rnd_rouge['rouge-2'],rnd_rouge['rouge-l'],rnd_bleu,rnd_meteor,rnd_bert_score],\n","    [\"Lead-3\",lead_rouge['rouge-1'],lead_rouge['rouge-2'],lead_rouge['rouge-l'],lead_bleu,lead_meteor,lead_bert_score],\n","    [\"TextRank\",tr_rouge['rouge-1'],tr_rouge['rouge-2'],tr_rouge['rouge-l'],tr_bleu,tr_meteor,tr_bert_score],\n","    [\"Oracle\",oracle_rouge['rouge-1'],oracle_rouge['rouge-2'],oracle_rouge['rouge-l'],oracle_bleu,oracle_meteor,oracle_bert_score],\n","    [\"BertSum\",bertsum_rouge['rouge-1'],bertsum_rouge['rouge-2'],bertsum_rouge['rouge-l'],bertsum_bleu,bertsum_meteor,bertsum_bert_score],\n","    [\"MatchSum\",matchsum_rouge['rouge-1'],matchsum_rouge['rouge-2'],matchsum_rouge['rouge-l'],matchsum_bleu,matchsum_meteor,matchsum_bert_score]\n","], columns=[\"Summary\",\"ROUGE-1\",\"ROUGE-2\",\"ROUGE-L\",\"BLEU\",\"METEOR\",\"BERT-Score\"])\n","\n","print(mlsum_eval_df)\n","mlsum_eval_df.to_csv(\"results/mlsum/eval.csv\", index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Iqbwyzp1af8"},"source":["## Summary Selection for Human Evaluation"]},{"cell_type":"code","metadata":{"id":"64J4yIz-oWAW"},"source":["import random\n","\n","short_summ_idx = [key for key, value in enumerate(mlsum_dataset[\"text_word_len\"]) if value>500 and value<=600]\n","long_summ_idx = [key for key, value in enumerate(mlsum_dataset[\"text_word_len\"]) if value>900 and value<=1100]\n","\n","print(len(short_summ_idx))\n","print(len(long_summ_idx))\n","\n","random_short_idx = random.sample(short_summ_idx, 15)\n","random_long_idx = random.sample(long_summ_idx, 15)\n","\n","#print(len(random_short_idx))\n","#print(len(random_long_idx))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NNYb05o6zAT-"},"source":["rnd_short_summ_list = [(value+2, mlsum_src[value], mlsum_tgt[value], mlsum_rnd_sum[value], mlsum_lead_sum[value], mlsum_textrank_sum[value]) for value in random_short_idx]\n","rnd_long_summ_list = [(value+2, mlsum_src[value], mlsum_tgt[value], mlsum_rnd_sum[value], mlsum_lead_sum[value], mlsum_textrank_sum[value]) for value in random_long_idx]\n","\n","#print(len(rnd_summ_list))\n","human_eval_short = pd.DataFrame(rnd_short_summ_list, columns=['index', 'source', 'expert', 'random', 'lead', 'textrank'])\n","human_eval_long = pd.DataFrame(rnd_long_summ_list, columns=['index', 'source', 'expert', 'random', 'lead', 'textrank'])\n","\n","human_eval_short.to_csv(\"results/mlsum/human_eval_short.csv\", index=False)\n","human_eval_long.to_csv(\"results/mlsum/human_eval_long.csv\", index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vswl2mlurXdJ"},"source":["import plotly.express as px\n","\n","fig1 = px.histogram(mlsum_dataset, x=\"text_word_len\", nbins=100, range_x=[101, 1200], labels={'text_word_len':'No. of words in source article'})\n","fig1.show()\n","\n","fig2 = px.histogram(mlsum_dataset, x=\"sum_word_len\", nbins=50, range_x=[1,50], labels={'sum_word_len':'No. of words in expert summary'})\n","fig2.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sxkixNu5vZsy"},"source":["# GeWiki"]},{"cell_type":"markdown","metadata":{"id":"mdsh10r_3Ozk"},"source":["## Data Loading"]},{"cell_type":"code","metadata":{"id":"cC_OomgXvgif"},"source":["# fetch GeWiki data splits from their github repo: https://github.com/domfr/GeWiki\n","\n","# Uncomment below lines, to fetch the GeWiki data, unzipping it, and merging mutiple files into one based on \"src\" or \"tgt\" for train, eval and test splits\n","\n","#!wget -nv -i data/gewiki_urls.txt -O data/gewiki/gewiki.zip\n","#!unzip data/gewiki/gewiki.zip -d data/gewiki/\n","#!awk 'BEGINFILE {print \"[SEP]\"}{print}' data/gewiki/test/*.src > data/gewiki/test_src.txt\n","#!awk 'BEGINFILE {print \"[SEP]\"}{print}' data/gewiki/test/*.tgt > data/gewiki/test_tgt.txt\n","#!awk 'BEGINFILE {print \"[SEP]\"}{print}' data/gewiki/eval/*.src > data/gewiki/validation_src.txt\n","#!awk 'BEGINFILE {print \"[SEP]\"}{print}' data/gewiki/eval/*.tgt > data/gewiki/validation_tgt.txt\n","#!awk 'BEGINFILE {print \"[SEP]\"}{print}' data/gewiki/train/*.src > data/gewiki/train_src.txt\n","#!awk 'BEGINFILE {print \"[SEP]\"}{print}' data/gewiki/train/*.tgt > data/gewiki/train_tgt.txt\n","\n","from collections import defaultdict\n","from tqdm import tqdm\n","import pandas as pd\n","\n","def merge_src_tgt_to_csv(num_of_files: int, csv_name: str) -> None:\n","    results = defaultdict(list)\n","\n","    with open(f\"data/gewiki/{csv_name}_src.txt\", \"r\") as src:\n","        src = src.read()\n","    with open(f\"data/gewiki/{csv_name}_tgt.txt\", \"r\") as tgt:\n","        tgt = tgt.read()\n","\n","    src_list = src.split(\"[SEP]\")\n","    tgt_list = tgt.split(\"[SEP]\")\n","    for i in tqdm(range(1, num_of_files + 1)):\n","        results[\"text\"].append(src_list[i])\n","        results[\"summary\"].append(tgt_list[i])\n","    df = pd.DataFrame(results)\n","    df.to_csv(f\"data/gewiki/{csv_name}.csv\", False)\n","\n","# Creating Train CSV\n","# merge_src_tgt_to_csv(220000, \"train\")\n","\n","# Creating Eval CSV\n","# merge_src_tgt_to_csv(10000, \"valid\")\n","\n","# Creating Test CSV\n","# merge_src_tgt_to_csv(10000, \"test\")\n","\n","#for corpus_type in ['train', 'valid', 'test']:\n","#    gewiki_dataset = pd.read_csv(f\"data/gewiki/{corpus_type}.csv\")\n","#    data_prep_for_BertSum(gewiki_dataset, 'german', \"json_data/gewiki\", corpus_type)\n","#    data_prep_for_MatchSum(gewiki_dataset, 'german', \"json_data/gewiki\", corpus_type)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"60gCoXOC2wnU","executionInfo":{"status":"ok","timestamp":1625523870825,"user_tz":-120,"elapsed":175248,"user":{"displayName":"Zohaib Akhtar Khan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDxR5SUI_CQuZ4d1iYZsq0ZqAkxd2KNHJ9ahkuVg=s64","userId":"11760862558786327949"}},"outputId":"270be208-ed79-458f-c546-577c38bf8d3a"},"source":["import pandas as pd\n","\n","# generating summaries only for the test set\n","gewiki_dataset = pd.read_csv(\"data/gewiki/test.csv\").iterrows()\n","gewiki_src, gewiki_rnd_sum, gewiki_lead_sum, gewiki_textrank_sum, gewiki_tgt = map(list,zip(*[(\n","                                                      get_text_with_breaks(row['text']),\n","                                                      get_random_summary(row['text']),\n","                                                      get_lead_summary(row['text']),\n","                                                      get_textrank_summary(get_text_with_breaks(row['text']), 0.1),\n","                                                      get_text_with_breaks(row['summary'])\n","                                                    ) for index, row in gewiki_dataset]))\n","print(len(gewiki_src))\n","print(len(gewiki_rnd_sum))\n","print(len(gewiki_lead_sum))\n","print(len(gewiki_textrank_sum))\n","print(len(gewiki_tgt))\n","\n","#pd.DataFrame(gewiki_rnd_sum).to_csv(\"results/gewiki/random_hypo.csv\", index=False)\n","#pd.DataFrame(gewiki_lead_sum).to_csv(\"results/gewiki/lead_hypo.csv\", index=False)\n","#pd.DataFrame(gewiki_textrank_sum).to_csv(\"results/gewiki/textrank_hypo.csv\", index=False)\n","#pd.DataFrame(gewiki_tgt).to_csv(\"results/gewiki/reference.csv\", index=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["10000\n","10000\n","10000\n","10000\n","10000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sXnQDnniMCCT","executionInfo":{"status":"ok","timestamp":1625523871174,"user_tz":-120,"elapsed":365,"user":{"displayName":"Zohaib Akhtar Khan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDxR5SUI_CQuZ4d1iYZsq0ZqAkxd2KNHJ9ahkuVg=s64","userId":"11760862558786327949"}},"outputId":"2d6d1be8-ba21-4c59-d83f-d1779f7e96aa"},"source":["import pandas as pd\n","\n","gewiki_oracle_sum = pd.read_csv(\"results/gewiki/oracle_hypo.csv\")['hypothesis'].fillna('').tolist()\n","gewiki_oracle_tgt = pd.read_csv(\"results/gewiki/oracle_ref.csv\")['references'].fillna('').tolist()\n","gewiki_bertsum_sum = pd.read_csv(\"results/gewiki/bertsum_hypo.csv\")['hypothesis'].fillna('').tolist()\n","gewiki_bertsum_tgt = pd.read_csv(\"results/gewiki/bertsum_ref.csv\")['references'].fillna('').tolist()\n","gewiki_matchsum_sum = pd.read_csv(\"results/gewiki/matchsum_hypo.csv\")['hypothesis'].fillna('').tolist()\n","gewiki_matchsum_tgt = pd.read_csv(\"results/gewiki/matchsum_ref.csv\")['references'].fillna('').tolist()\n","\n","print(len(gewiki_oracle_sum))\n","print(len(gewiki_oracle_tgt))\n","print(len(gewiki_bertsum_sum))\n","print(len(gewiki_bertsum_tgt))\n","print(len(gewiki_matchsum_sum))\n","print(len(gewiki_matchsum_tgt))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["10000\n","10000\n","10000\n","10000\n","10000\n","10000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cj1XETJ76sly"},"source":["## Individual evaluation results per evaluation method"]},{"cell_type":"code","metadata":{"id":"N5C-9TfZ6yq_"},"source":["import pandas as pd\n","\n","r1_rnd, r2_rnd, rl_rnd = get_rouge(gewiki_rnd_sum, gewiki_tgt, avg=False, ignore_empty=False)\n","r1_lead, r2_lead, rl_lead = get_rouge(gewiki_lead_sum, gewiki_tgt, avg=False, ignore_empty=False)\n","r1_tr, r2_tr, rl_tr = get_rouge(gewiki_textrank_sum, gewiki_tgt, avg=False, ignore_empty=False)\n","r1_bs, r2_bs, rl_bs = get_rouge(gewiki_bertsum_sum, gewiki_bertsum_tgt, avg=False, ignore_empty=False)\n","r1_ms, r2_ms, rl_ms = get_rouge(gewiki_matchsum_sum, gewiki_matchsum_tgt, avg=False, ignore_empty=False)\n","r1_oracle, r2_oracle, rl_oracle = get_rouge(gewiki_oracle_sum, gewiki_oracle_tgt, avg=False, ignore_empty=False)\n","\n","rouge_eval = pd.DataFrame({\n","    'r1-Random-3': r1_rnd, 'r2-Random-3': r2_rnd, 'rl-Random-3': rl_rnd,\n","    'r1-Lead-3': r1_lead, 'r2-Lead-3': r2_lead, 'rl-Lead-3': rl_lead,\n","    'r1-TextRank': r1_tr, 'r2-TextRank': r2_tr, 'rl-TextRank': rl_tr,\n","    'r1-BertSum': r1_bs, 'r2-BertSum': r2_bs, 'rl-BertSum': rl_bs,\n","    'r1-MatchSum': r1_ms, 'r2-Matchsum': r2_ms, 'rl-MatchSum': rl_ms,\n","    'r1-Oracle': r1_oracle, 'r2-Oracle': r2_oracle, 'rl-Oracle': rl_oracle\n","})\n","rouge_eval.to_csv(\"results/gewiki/rouge_eval.csv\", index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1iKcdfTu79yd"},"source":["bleu_eval = pd.DataFrame({\n","    'Random-3': get_bleu(gewiki_rnd_sum, gewiki_tgt, avg=False),\n","    'Lead-3': get_bleu(gewiki_lead_sum, gewiki_tgt, avg=False),\n","    'TextRank': get_bleu(gewiki_textrank_sum, gewiki_tgt, avg=False),\n","    'BertSum': get_bleu(gewiki_bertsum_sum, gewiki_bertsum_tgt, avg=False),\n","    'MatchSum': get_bleu(gewiki_matchsum_sum, gewiki_matchsum_tgt, avg=False),\n","    'Oracle': get_bleu(gewiki_oracle_sum, gewiki_oracle_tgt, avg=False)\n","})\n","bleu_eval.to_csv(\"results/gewiki/bleu_eval.csv\", index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1CE6d9UM8EP6"},"source":["meteor_eval = pd.DataFrame({\n","    'Random-3': get_meteor(gewiki_rnd_sum, gewiki_tgt, avg=False),\n","    'Lead-3': get_meteor(gewiki_lead_sum, gewiki_tgt, avg=False),\n","    'TextRank': get_meteor(gewiki_textrank_sum, gewiki_tgt, avg=False),\n","    'BertSum': get_meteor(gewiki_bertsum_sum, gewiki_bertsum_tgt, avg=False),\n","    'MatchSum': get_meteor(gewiki_matchsum_sum, gewiki_matchsum_tgt, avg=False),\n","    'Oracle': get_meteor(gewiki_oracle_sum, gewiki_oracle_tgt, avg=False)\n","})\n","meteor_eval.to_csv(\"results/gewiki/meteor_eval.csv\", index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KTWLCsAm8LvS"},"source":["bert_score_eval = pd.DataFrame({\n","    'Random-3': get_bert_score(gewiki_rnd_sum, gewiki_tgt, avg=False),\n","    'Lead-3': get_bert_score(gewiki_lead_sum, gewiki_tgt, avg=False),\n","    'TextRank': get_bert_score(gewiki_textrank_sum, gewiki_tgt, avg=False),\n","    'BertSum': get_bert_score(gewiki_bertsum_sum, gewiki_bertsum_tgt, avg=False),\n","    'MatchSum': get_bert_score(gewiki_matchsum_sum, gewiki_matchsum_tgt, avg=False),\n","    'Oracle': get_bert_score(gewiki_oracle_sum, gewiki_oracle_tgt, avg=False)\n","})\n","bert_score_eval.to_csv(\"results/gewiki/bert_score_eval.csv\", index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"D9K7TJEzh2iK"},"source":["mover_score_eval = pd.DataFrame({\n","    'Random-3': get_mover_score(gewiki_rnd_sum, gewiki_tgt, avg=False),\n","    'Lead-3': get_mover_score(gewiki_lead_sum, gewiki_tgt, avg=False),\n","    'TextRank': get_mover_score(gewiki_textrank_sum, gewiki_tgt, avg=False),\n","    'BertSum': get_mover_score(gewiki_bertsum_sum, gewiki_bertsum_tgt, avg=False),\n","    'MatchSum': get_mover_score(gewiki_matchsum_sum, gewiki_matchsum_tgt, avg=False),\n","    'Oracle': get_mover_score(gewiki_oracle_sum, gewiki_oracle_tgt, avg=False)\n","})\n","mover_score_eval.to_csv(\"results/gewiki/mover_score_eval.csv\", index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RV1E5Xai8gCK"},"source":["blanc_eval = pd.DataFrame({\n","    'Random-3': get_blanc(gewiki_rnd_sum, gewiki_src, avg=False),\n","    'Lead-3': get_blanc(gewiki_lead_sum, gewiki_src, avg=False),\n","    'TextRank': get_blanc(gewiki_textrank_sum, gewiki_src, avg=False),\n","    'BertSum': get_blanc(gewiki_bertsum_sum, gewiki_src, avg=False),\n","    'MatchSum': get_blanc(gewiki_matchsum_sum, gewiki_src, avg=False),\n","    'Oracle': get_blanc(gewiki_oracle_sum, gewiki_src, avg=False)\n","})\n","blanc_eval.to_csv(\"results/gewiki/blanc_eval.csv\", index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VqF-q88tiNB5","executionInfo":{"status":"ok","timestamp":1625441427945,"user_tz":-120,"elapsed":389957,"user":{"displayName":"Zohaib Akhtar Khan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDxR5SUI_CQuZ4d1iYZsq0ZqAkxd2KNHJ9ahkuVg=s64","userId":"11760862558786327949"}},"outputId":"e02da425-df8f-4105-bd0d-627ad91ff114"},"source":["js_eval = pd.DataFrame({\n","    'Random-3': get_jensenshannon(gewiki_rnd_sum, gewiki_src, avg=False),\n","    'Lead-3': get_jensenshannon(gewiki_lead_sum, gewiki_src, avg=False),\n","    'TextRank': get_jensenshannon(gewiki_textrank_sum, gewiki_src, avg=False),\n","    'BertSum': get_jensenshannon(gewiki_bertsum_sum, gewiki_src, avg=False),\n","    'MatchSum': get_jensenshannon(gewiki_matchsum_sum, gewiki_src, avg=False),\n","    'Oracle': get_jensenshannon(gewiki_oracle_sum, gewiki_src, avg=False)\n","})\n","js_eval.to_csv(\"results/gewiki/js_eval.csv\", index=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/scipy/spatial/distance.py:1288: RuntimeWarning: invalid value encountered in true_divide\n","  p = p / np.sum(p, axis=0)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9MjUUCNBk5O5","executionInfo":{"status":"ok","timestamp":1625530351525,"user_tz":-120,"elapsed":6480356,"user":{"displayName":"Zohaib Akhtar Khan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDxR5SUI_CQuZ4d1iYZsq0ZqAkxd2KNHJ9ahkuVg=s64","userId":"11760862558786327949"}},"outputId":"93018213-8bdd-4cf5-9aaf-30e0d1ac861c"},"source":["supert_eval = pd.read_csv(\"results/gewiki/supert_eval.csv\")\n","% cd /usr/local/lib/python3.7/dist-packages/summ_eval/\n","#supert_eval['BertSum'] = get_supert(gewiki_bertsum_sum, gewiki_src, avg=False)\n","supert_eval['MatchSum'] = get_supert(gewiki_matchsum_sum, gewiki_src, avg=False)\n","supert_eval['Oracle'] = get_supert(gewiki_oracle_sum, gewiki_src, avg=False)\n","% cd /content/drive/My Drive/GeSumGenEval\n","supert_eval.to_csv(\"results/gewiki/supert_eval.csv\", index=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/summ_eval\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /root/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-large-nli-stsb-mean-tokens.zip/0_BERT were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at /root/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-large-nli-stsb-mean-tokens.zip/0_BERT were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"},{"output_type":"stream","text":["/content/drive/.shortcut-targets-by-id/1PcWXs_So5sTaP0wBAR77_ORSfd4aFtHq/GeSumGenEval\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Jea3PexD5Su7"},"source":["% cd /usr/local/lib/python3.7/dist-packages/summ_eval/\n","supert_eval = pd.DataFrame({\n","    'Random-3': get_supert(gewiki_rnd_sum, gewiki_src, avg=False),\n","    'Lead-3': get_supert(gewiki_lead_sum, gewiki_src, avg=False),\n","    'TextRank': get_supert(gewiki_textrank_sum, gewiki_src, avg=False),\n","    'BertSum': get_jensenshannon(gewiki_bertsum_sum, gewiki_src, avg=False),\n","    'MatchSum': get_jensenshannon(gewiki_matchsum_sum, gewiki_src, avg=False),\n","    'Oracle': get_jensenshannon(gewiki_oracle_sum, gewiki_src, avg=False)\n","})\n","% cd /content/drive/My Drive/GeSumGenEval\n","supert_eval.to_csv(\"results/gewiki/supert_eval.csv\", index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZJelBJKq4HmL"},"source":["## Aggregate evaluation results per generation method"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aB6rQjVp5nxu","executionInfo":{"elapsed":7,"status":"ok","timestamp":1625344412986,"user":{"displayName":"Zohaib Akhtar Khan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDxR5SUI_CQuZ4d1iYZsq0ZqAkxd2KNHJ9ahkuVg=s64","userId":"11760862558786327949"},"user_tz":-120},"outputId":"f7562269-0d8e-4069-8ef6-70d23303dd3c"},"source":["% cd /usr/local/lib/python3.7/dist-packages/summ_eval/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/summ_eval\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SQ49VjoSMC8I","executionInfo":{"elapsed":3113785,"status":"ok","timestamp":1625347527154,"user":{"displayName":"Zohaib Akhtar Khan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDxR5SUI_CQuZ4d1iYZsq0ZqAkxd2KNHJ9ahkuVg=s64","userId":"11760862558786327949"},"user_tz":-120},"outputId":"c60935ea-e1c8-4be9-92c5-a38820f066a1"},"source":["#oracle_rouge = get_rouge(gewiki_oracle_sum, gewiki_oracle_tgt)\n","#oracle_bleu = get_bleu(gewiki_oracle_sum, gewiki_oracle_tgt)\n","#oracle_meteor = get_meteor(gewiki_oracle_sum, gewiki_oracle_tgt)\n","#oracle_bert_score = get_bert_score(gewiki_oracle_sum, gewiki_oracle_tgt)\n","#oracle_mover_score = get_mover_score(gewiki_oracle_sum, gewiki_oracle_tgt)\n","#oracle_blanc = get_blanc(gewiki_oracle_sum, gewiki_src)\n","#oracle_js = get_jensenshannon(gewiki_oracle_sum, gewiki_src)\n","oracle_supert = get_supert(gewiki_oracle_sum, gewiki_src)\n","print(oracle_supert)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|| 1.24G/1.24G [00:31<00:00, 39.1MB/s]\n","Some weights of the model checkpoint at /root/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-large-nli-stsb-mean-tokens.zip/0_BERT were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"},{"output_type":"stream","text":["0.7557766571427685\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bLzKspQ6MDKW","executionInfo":{"elapsed":3840993,"status":"ok","timestamp":1625351368137,"user":{"displayName":"Zohaib Akhtar Khan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDxR5SUI_CQuZ4d1iYZsq0ZqAkxd2KNHJ9ahkuVg=s64","userId":"11760862558786327949"},"user_tz":-120},"outputId":"2c40ed97-c94c-47b4-ef62-d2647b52e90d"},"source":["#bertsum_rouge = get_rouge(gewiki_bertsum_sum, gewiki_bertsum_tgt)\n","#bertsum_bleu = get_bleu(gewiki_bertsum_sum, gewiki_bertsum_tgt)\n","#bertsum_meteor = get_meteor(gewiki_bertsum_sum, gewiki_bertsum_tgt)\n","#bertsum_bert_score = get_bert_score(gewiki_bertsum_sum, gewiki_bertsum_tgt)\n","#bertsum_mover_score = get_mover_score(gewiki_bertsum_sum, gewiki_bertsum_tgt)\n","#bertsum_blanc = get_blanc(gewiki_bertsum_sum, gewiki_src)\n","#bertsum_js = get_jensenshannon(gewiki_bertsum_sum, gewiki_src)\n","bertsum_supert = get_supert(gewiki_bertsum_sum, gewiki_src)\n","print(bertsum_supert)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at /root/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-large-nli-stsb-mean-tokens.zip/0_BERT were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"},{"output_type":"stream","text":["0.8008041634642045\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3jbxJKlA41YJ","executionInfo":{"elapsed":3767163,"status":"ok","timestamp":1625355136515,"user":{"displayName":"Zohaib Akhtar Khan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDxR5SUI_CQuZ4d1iYZsq0ZqAkxd2KNHJ9ahkuVg=s64","userId":"11760862558786327949"},"user_tz":-120},"outputId":"1cb30d7a-24e0-4de7-9ec3-50c9eaf04fb3"},"source":["#matchsum_rouge = get_rouge(gewiki_matchsum_sum, gewiki_matchsum_tgt)\n","#matchsum_bleu = get_bleu(gewiki_matchsum_sum, gewiki_matchsum_tgt)\n","#matchsum_meteor = get_meteor(gewiki_matchsum_sum, gewiki_matchsum_tgt)\n","#matchsum_bert_score = get_bert_score(gewiki_matchsum_sum, gewiki_matchsum_tgt)\n","#matchsum_mover_score = get_mover_score(gewiki_matchsum_sum, gewiki_matchsum_tgt)\n","#matchsum_blanc = get_blanc(gewiki_matchsum_sum, gewiki_src)\n","#matchsum_js = get_jensenshannon(gewiki_matchsum_sum, gewiki_src)\n","matchsum_supert = get_supert(gewiki_matchsum_sum, gewiki_src)\n","print(matchsum_supert)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at /root/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-large-nli-stsb-mean-tokens.zip/0_BERT were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"},{"output_type":"stream","text":["0.8000698033867725\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t-iCuHlo4NB3","executionInfo":{"elapsed":3432731,"status":"ok","timestamp":1624316593615,"user":{"displayName":"Zohaib Akhtar Khan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDxR5SUI_CQuZ4d1iYZsq0ZqAkxd2KNHJ9ahkuVg=s64","userId":"11760862558786327949"},"user_tz":-120},"outputId":"ef3d5c4c-3838-41fb-f3f0-f0ef54da65c5"},"source":["#rnd_rouge = get_rouge(gewiki_rnd_sum, gewiki_tgt)\n","#rnd_bleu = get_bleu(gewiki_rnd_sum, gewiki_tgt)\n","#rnd_meteor = get_meteor(gewiki_rnd_sum, gewiki_tgt)\n","#rnd_bert_score = get_bert_score(gewiki_rnd_sum, gewiki_tgt)\n","#rnd_mover_score = get_mover_score(gewiki_rnd_sum, gewiki_tgt)\n","#rnd_js = get_jensenshannon(gewiki_rnd_sum, gewiki_src)\n","rnd_supert = get_supert(gewiki_rnd_sum, gewiki_src)\n","print(rnd_supert)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at /root/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-large-nli-stsb-mean-tokens.zip/0_BERT were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"},{"output_type":"stream","text":["0.7554327657610459\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FLPASq0_4Pxt","executionInfo":{"elapsed":3414738,"status":"ok","timestamp":1624320008341,"user":{"displayName":"Zohaib Akhtar Khan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDxR5SUI_CQuZ4d1iYZsq0ZqAkxd2KNHJ9ahkuVg=s64","userId":"11760862558786327949"},"user_tz":-120},"outputId":"9c098afd-dcda-4419-f91c-6184462f7b6b"},"source":["#lead_rouge = get_rouge(gewiki_lead_sum, gewiki_tgt)\n","#lead_bleu = get_bleu(gewiki_lead_sum, gewiki_tgt)\n","#lead_meteor = get_meteor(gewiki_lead_sum, gewiki_tgt)\n","#lead_bert_score = get_bert_score(gewiki_lead_sum, gewiki_tgt)\n","#lead_mover_score = get_mover_score(gewiki_lead_sum, gewiki_tgt)\n","#lead_js = get_jensenshannon(gewiki_lead_sum, gewiki_src)\n","lead_supert = get_supert(gewiki_lead_sum, gewiki_src)\n","print(lead_supert)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at /root/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-large-nli-stsb-mean-tokens.zip/0_BERT were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"},{"output_type":"stream","text":["0.8266020193139256\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EWKvg54S4Qib","executionInfo":{"elapsed":3403969,"status":"ok","timestamp":1624323412299,"user":{"displayName":"Zohaib Akhtar Khan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDxR5SUI_CQuZ4d1iYZsq0ZqAkxd2KNHJ9ahkuVg=s64","userId":"11760862558786327949"},"user_tz":-120},"outputId":"7215f291-634b-4dcd-ab07-8d9561986517"},"source":["#tr_rouge = get_rouge(gewiki_textrank_sum, gewiki_tgt)\n","#tr_bleu = get_bleu(gewiki_textrank_sum, gewiki_tgt)\n","#tr_meteor = get_meteor(gewiki_textrank_sum, gewiki_tgt)\n","#tr_bert_score = get_bert_score(gewiki_textrank_sum, gewiki_tgt)\n","#tr_mover_score = get_mover_score(gewiki_textrank_sum, gewiki_tgt)\n","#tr_blanc = get_blanc(gewiki_textrank_sum, gewiki_src)\n","#tr_js = get_jensenshannon(gewiki_textrank_sum, gewiki_src)\n","tr_supert = get_supert(gewiki_textrank_sum, gewiki_src)\n","print(tr_supert)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at /root/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-large-nli-stsb-mean-tokens.zip/0_BERT were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"},{"output_type":"stream","text":["0.7485024953768681\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L1T45sJv-2QP","executionInfo":{"elapsed":213,"status":"ok","timestamp":1623734962193,"user":{"displayName":"Zohaib Akhtar Khan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDxR5SUI_CQuZ4d1iYZsq0ZqAkxd2KNHJ9ahkuVg=s64","userId":"11760862558786327949"},"user_tz":-120},"outputId":"9065ab56-60b1-40c4-e6db-297fd562eebb"},"source":["import pandas as pd\n","\n","gewiki_eval_df = pd.DataFrame([\n","    [\"Random-3\",rnd_rouge['rouge-1'],rnd_rouge['rouge-2'],rnd_rouge['rouge-l'],rnd_bleu,rnd_meteor,rnd_bert_score],\n","    [\"Lead-3\",lead_rouge['rouge-1'],lead_rouge['rouge-2'],lead_rouge['rouge-l'],lead_bleu,lead_meteor,lead_bert_score],\n","    [\"TextRank\",tr_rouge['rouge-1'],tr_rouge['rouge-2'],tr_rouge['rouge-l'],tr_bleu,tr_meteor,tr_bert_score],\n","    [\"Oracle\",oracle_rouge['rouge-1'],oracle_rouge['rouge-2'],oracle_rouge['rouge-l'],oracle_bleu,oracle_meteor,oracle_bert_score],\n","    [\"BertSum\",bertsum_rouge['rouge-1'],bertsum_rouge['rouge-2'],bertsum_rouge['rouge-l'],bertsum_bleu,bertsum_meteor,bertsum_bert_score],\n","    [\"MatchSum\",matchsum_rouge['rouge-1'],matchsum_rouge['rouge-2'],matchsum_rouge['rouge-l'],matchsum_bleu,matchsum_meteor,matchsum_bert_score]\n","], columns=[\"Summary\",\"ROUGE-1\",\"ROUGE-2\",\"ROUGE-L\",\"BLEU\",\"METEOR\",\"BERT-Score\"])\n","\n","print(gewiki_eval_df)\n","gewiki_eval_df.to_csv(\"results/gewiki/eval.csv\", index=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["    Summary   ROUGE-1   ROUGE-2   ROUGE-L      BLEU    METEOR  BERT-Score\n","0  Random-3  0.186970  0.061312  0.148257  2.353785  0.118363    0.568395\n","1    Lead-3  0.212807  0.076299  0.167209  2.719979  0.127272    0.587119\n","2  TextRank  0.237438  0.086741  0.176155  2.761369  0.137756    0.569675\n","3    Oracle  0.383840  0.201094  0.306631  8.574861  0.140905    0.576921\n","4   BertSum  0.288809  0.125421  0.223787  4.829666  0.153491    0.624984\n","5  MatchSum  0.252191  0.100162  0.202067  3.975334  0.128689    0.608604\n"],"name":"stdout"}]}]}